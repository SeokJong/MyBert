{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import nltk\n",
    "import collections\n",
    "import unicodedata\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'gzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'tail' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# LANG_CODE = \"en\" #@param {type:\"string\"}\n",
    "\n",
    "# !wget http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2016/mono/OpenSubtitles.raw.'$LANG_CODE'.gz -O dataset.txt.gz\n",
    "# !gzip -d dataset.txt.gz\n",
    "# !tail dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO_MODE = True #@param {type:\"boolean\"}\n",
    "\n",
    "# if DEMO_MODE:\n",
    "#   CORPUS_SIZE = 1000000\n",
    "# else:\n",
    "#   CORPUS_SIZE = 100000000 #@param {type: \"integer\"}\n",
    "  \n",
    "# !(head -n $CORPUS_SIZE dataset.txt) > subdataset.txt\n",
    "# !mv subdataset.txt dataset.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = nltk.RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "def normalize_text(text):\n",
    "  # lowercase text\n",
    "  text = str(text).lower()\n",
    "  # remove non-UTF\n",
    "  text = text.encode(\"utf-8\", \"ignore\").decode()\n",
    "  # remove punktuation symbols\n",
    "  text = \" \".join(regex_tokenizer.tokenize(text))\n",
    "  return text\n",
    "\n",
    "def count_lines(filename):\n",
    "  count = 0\n",
    "  with open(filename) as fi:\n",
    "    for line in fi:\n",
    "      count += 1\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000/5000000 [==============================] - 19s 4us/step\n",
      "5000001/5000000 [==============================] - 19s 4us/step\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA_FPATH = \"dataset.txt\" #@param {type: \"string\"}\n",
    "PRC_DATA_FPATH = \"proc_dataset.txt\" #@param {type: \"string\"}\n",
    "\n",
    "# apply normalization to the dataset\n",
    "# this will take a minute or two\n",
    "total_line = 5000000\n",
    "# total_lines = count_lines(RAW_DATA_FPATH)\n",
    "bar = Progbar(total_line)\n",
    "\n",
    "with open(RAW_DATA_FPATH,encoding=\"utf-8\") as fi:\n",
    "  with open(PRC_DATA_FPATH, \"w\",encoding=\"utf-8\") as fo:\n",
    "    for i, l in enumerate(fi):\n",
    "      fo.write(normalize_text(l)+\"\\n\")\n",
    "      bar.add(1)\n",
    "      if i == total_line:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PREFIX = \"tokenizer\"\n",
    "VOC_SIZE = 32000\n",
    "SUBSAMPLE_SIZE = 12800000\n",
    "NUM_PLACEHOLDERS = 256\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
    "               '--vocab_size={} --input_sentence_size={} '\n",
    "               '--shuffle_input_sentence=true ' \n",
    "               '--bos_id=-1 --eos_id=-1').format(\n",
    "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
    "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
    "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learnt vocab size: 31743\n",
      "Sample tokens: ['▁beginner', '▁fault', '▁experiment', 'inian', '▁clumsi', 'uigg', '▁measure', 'dark', '▁southwest', '▁rapper']\n"
     ]
    }
   ],
   "source": [
    "def read_sentencepiece_vocab(filepath):\n",
    "  voc = []\n",
    "  with open(filepath, encoding='utf-8') as fi:\n",
    "    for line in fi:\n",
    "      voc.append(line.split(\"\\t\")[0])\n",
    "  # skip the first <unk> token\n",
    "  voc = voc[1:]\n",
    "  return voc\n",
    "\n",
    "snt_vocab = read_sentencepiece_vocab(\"{}.vocab\".format(MODEL_PREFIX))\n",
    "print(\"Learnt vocab size: {}\".format(len(snt_vocab)))\n",
    "print(\"Sample tokens: {}\".format(random.sample(snt_vocab, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentencepiece_token(token):\n",
    "    if token.startswith(\"▁\"):\n",
    "        return token[1:]\n",
    "    else:\n",
    "        return \"##\" + token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))\n",
    "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
    "bert_vocab = ctrl_symbols + bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "bert_vocab += [\"[UNUSED_{}]\".format(i) for i in range(VOC_SIZE - len(bert_vocab))]\n",
    "print(len(bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
    "\n",
    "with open(VOC_FNAME, \"w\") as fo:\n",
    "  for token in bert_vocab:\n",
    "    fo.write(token+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./shards\n",
    "!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_\n",
    "!ls ./shards/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리 (create_pretraining_data)\n",
    "\n",
    "XARGS_CMD = (\"ls ./shards/ | \"\n",
    "             \"xargs -n 1 -P {} -I{} \"\n",
    "             \"python3 bert/create_pretraining_data.py \"\n",
    "             \"--input_file=./shards/{} \"\n",
    "             \"--output_file={}/{}.tfrecord \"\n",
    "             \"--vocab_file={} \"\n",
    "             \"--do_lower_case={} \"\n",
    "             \"--max_predictions_per_seq={} \"\n",
    "             \"--max_seq_length={} \"\n",
    "             \"--masked_lm_prob={} \"\n",
    "             \"--random_seed=34 \"\n",
    "             \"--dupe_factor=5\")\n",
    "\n",
    "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', \n",
    "                             VOC_FNAME, DO_LOWER_CASE, \n",
    "                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)\n",
    "\n",
    "tf.gfile.MkDir(PRETRAINING_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
    "MASKED_LM_PROB = 0.15 #@param\n",
    "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
    "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
    "PROCESSES = 2 #@param {type:\"integer\"}\n",
    "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}\n",
    "DUPE_FACTOR = 10\n",
    "SHROT_SEQ_PROB = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=VOC_FNAME, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pattern = \"./shards/*\"\n",
    "input_files = []\n",
    "input_files.extend(tf.io.gfile.glob(input_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Reading from input files ***\n",
      "2021-03-24 06:41:33,224 :    .\\shards\\shard_0000\n",
      "INFO:tensorflow:  .\\shards\\shard_0000\n",
      "2021-03-24 06:41:33,225 :    .\\shards\\shard_0001\n",
      "INFO:tensorflow:  .\\shards\\shard_0001\n",
      "2021-03-24 06:41:33,226 :    .\\shards\\shard_0002\n",
      "INFO:tensorflow:  .\\shards\\shard_0002\n",
      "2021-03-24 06:41:33,226 :    .\\shards\\shard_0003\n",
      "INFO:tensorflow:  .\\shards\\shard_0003\n",
      "2021-03-24 06:41:33,227 :    .\\shards\\shard_0004\n",
      "INFO:tensorflow:  .\\shards\\shard_0004\n",
      "2021-03-24 06:41:33,228 :    .\\shards\\shard_0005\n",
      "INFO:tensorflow:  .\\shards\\shard_0005\n",
      "2021-03-24 06:41:33,229 :    .\\shards\\shard_0006\n",
      "INFO:tensorflow:  .\\shards\\shard_0006\n",
      "2021-03-24 06:41:33,231 :    .\\shards\\shard_0007\n",
      "INFO:tensorflow:  .\\shards\\shard_0007\n",
      "2021-03-24 06:41:33,232 :    .\\shards\\shard_0008\n",
      "INFO:tensorflow:  .\\shards\\shard_0008\n",
      "2021-03-24 06:41:33,233 :    .\\shards\\shard_0009\n",
      "INFO:tensorflow:  .\\shards\\shard_0009\n",
      "2021-03-24 06:41:33,234 :    .\\shards\\shard_0010\n",
      "INFO:tensorflow:  .\\shards\\shard_0010\n",
      "2021-03-24 06:41:33,234 :    .\\shards\\shard_0011\n",
      "INFO:tensorflow:  .\\shards\\shard_0011\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"*** Reading from input files ***\")\n",
    "for input_file in input_files:\n",
    "    tf.compat.v1.logging.info(\"  %s\", input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_instances(input_files, tokenizer, max_seq_length,\n",
    "                              dupe_factor, short_seq_prob, masked_lm_prob,\n",
    "                              max_predictions_per_seq, rng):\n",
    "  \"\"\"Create `TrainingInstance`s from raw text.\"\"\"\n",
    "  all_documents = [[]]\n",
    "\n",
    "  # Input file format:\n",
    "  # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "  # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "  # sentence boundaries for the \"next sentence prediction\" task).\n",
    "  # (2) Blank lines between documents. Document boundaries are needed so\n",
    "  # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "  for input_file in input_files:\n",
    "    with tf.io.gfile.GFile(input_file, \"r\") as reader:\n",
    "      while True:\n",
    "        line = tokenization.convert_to_unicode(reader.readline())\n",
    "        if not line:\n",
    "          break\n",
    "        line = line.strip()\n",
    "\n",
    "        # Empty lines are used as document delimiters\n",
    "        if not line:\n",
    "          all_documents.append([])\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        if tokens:\n",
    "          all_documents[-1].append(tokens)\n",
    "\n",
    "  # Remove empty documents\n",
    "  all_documents = [x for x in all_documents if x]\n",
    "  rng.shuffle(all_documents)\n",
    "\n",
    "  vocab_words = list(tokenizer.vocab.keys())\n",
    "  instances = []\n",
    "  for _ in range(dupe_factor):\n",
    "    for document_index in range(len(all_documents)):\n",
    "      instances.extend(\n",
    "          create_instances_from_document(\n",
    "              all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))\n",
    "\n",
    "  rng.shuffle(instances)\n",
    "  return instances\n",
    "\n",
    "def create_instances_from_document(\n",
    "    all_documents, document_index, max_seq_length, short_seq_prob,\n",
    "    masked_lm_prob, max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates `TrainingInstance`s for a single document.\"\"\"\n",
    "  document = all_documents[document_index]\n",
    "\n",
    "  # Account for [CLS], [SEP], [SEP]\n",
    "  max_num_tokens = max_seq_length - 3\n",
    "\n",
    "  # We *usually* want to fill up the entire sequence since we are padding\n",
    "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "  # computation. However, we *sometimes*\n",
    "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "  # The `target_seq_length` is just a rough target however, whereas\n",
    "  # `max_seq_length` is a hard limit.\n",
    "  target_seq_length = max_num_tokens\n",
    "  if rng.random() < short_seq_prob:\n",
    "    target_seq_length = rng.randint(2, max_num_tokens)\n",
    "\n",
    "  # We DON'T just concatenate all of the tokens from a document into a long\n",
    "  # sequence and choose an arbitrary split point because this would make the\n",
    "  # next sentence prediction task too easy. Instead, we split the input into\n",
    "  # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "  # input.\n",
    "  instances = []\n",
    "  current_chunk = []\n",
    "  current_length = 0\n",
    "  i = 0\n",
    "  while i < len(document):\n",
    "    segment = document[i]\n",
    "    current_chunk.append(segment)\n",
    "    current_length += len(segment)\n",
    "    if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "      if current_chunk:\n",
    "        # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "        # (first) sentence.\n",
    "        a_end = 1\n",
    "        if len(current_chunk) >= 2:\n",
    "          a_end = rng.randint(1, len(current_chunk) - 1)\n",
    "\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "          tokens_a.extend(current_chunk[j])\n",
    "\n",
    "        tokens_b = []\n",
    "        # Random next\n",
    "        is_random_next = False\n",
    "        if len(current_chunk) == 1 or rng.random() < 0.5:\n",
    "          is_random_next = True\n",
    "          target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "          # This should rarely go for more than one iteration for large\n",
    "          # corpora. However, just to be careful, we try to make sure that\n",
    "          # the random document is not the same as the document\n",
    "          # we're processing.\n",
    "          for _ in range(10):\n",
    "            random_document_index = rng.randint(0, len(all_documents) - 1)\n",
    "            if random_document_index != document_index:\n",
    "              break\n",
    "\n",
    "          random_document = all_documents[random_document_index]\n",
    "          random_start = rng.randint(0, len(random_document) - 1)\n",
    "          for j in range(random_start, len(random_document)):\n",
    "            tokens_b.extend(random_document[j])\n",
    "            if len(tokens_b) >= target_b_length:\n",
    "              break\n",
    "          # We didn't actually use these segments so we \"put them back\" so\n",
    "          # they don't go to waste.\n",
    "          num_unused_segments = len(current_chunk) - a_end\n",
    "          i -= num_unused_segments\n",
    "        # Actual next\n",
    "        else:\n",
    "          is_random_next = False\n",
    "          for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)\n",
    "\n",
    "        assert len(tokens_a) >= 1\n",
    "        assert len(tokens_b) >= 1\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        segment_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(0)\n",
    "\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(0)\n",
    "\n",
    "        for token in tokens_b:\n",
    "          tokens.append(token)\n",
    "          segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "        (tokens, masked_lm_positions,\n",
    "         masked_lm_labels) = create_masked_lm_predictions(\n",
    "             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)\n",
    "        instance = TrainingInstance(\n",
    "            tokens=tokens,\n",
    "            segment_ids=segment_ids,\n",
    "            is_random_next=is_random_next,\n",
    "            masked_lm_positions=masked_lm_positions,\n",
    "            masked_lm_labels=masked_lm_labels)\n",
    "        instances.append(instance)\n",
    "      current_chunk = []\n",
    "      current_length = 0\n",
    "    i += 1\n",
    "\n",
    "  return instances\n",
    "\n",
    "MaskedLmInstance = collections.namedtuple(\"MaskedLmInstance\",\n",
    "                                          [\"index\", \"label\"])\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob,\n",
    "                                 max_predictions_per_seq, vocab_words, rng):\n",
    "  \"\"\"Creates the predictions for the masked LM objective.\"\"\"\n",
    "\n",
    "  cand_indexes = []\n",
    "  do_whole_word_mask = False\n",
    "  for (i, token) in enumerate(tokens):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "      continue\n",
    "    # Whole Word Masking means that if we mask all of the wordpieces\n",
    "    # corresponding to an original word. When a word has been split into\n",
    "    # WordPieces, the first token does not have any marker and any subsequence\n",
    "    # tokens are prefixed with ##. So whenever we see the ## token, we\n",
    "    # append it to the previous set of word indexes.\n",
    "    #\n",
    "    # Note that Whole Word Masking does *not* change the training code\n",
    "    # at all -- we still predict each WordPiece independently, softmaxed\n",
    "    # over the entire vocabulary.\n",
    "    if (do_whole_word_mask and len(cand_indexes) >= 1 and\n",
    "        token.startswith(\"##\")):\n",
    "      cand_indexes[-1].append(i)\n",
    "    else:\n",
    "      cand_indexes.append([i])\n",
    "\n",
    "  rng.shuffle(cand_indexes)\n",
    "\n",
    "  output_tokens = list(tokens)\n",
    "\n",
    "  num_to_predict = min(max_predictions_per_seq,\n",
    "                       max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "\n",
    "  masked_lms = []\n",
    "  covered_indexes = set()\n",
    "  for index_set in cand_indexes:\n",
    "    if len(masked_lms) >= num_to_predict:\n",
    "      break\n",
    "    # If adding a whole-word mask would exceed the maximum number of\n",
    "    # predictions, then just skip this candidate.\n",
    "    if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "      continue\n",
    "    is_any_index_covered = False\n",
    "    for index in index_set:\n",
    "      if index in covered_indexes:\n",
    "        is_any_index_covered = True\n",
    "        break\n",
    "    if is_any_index_covered:\n",
    "      continue\n",
    "    for index in index_set:\n",
    "      covered_indexes.add(index)\n",
    "\n",
    "      masked_token = None\n",
    "      # 80% of the time, replace with [MASK]\n",
    "      if rng.random() < 0.8:\n",
    "        masked_token = \"[MASK]\"\n",
    "      else:\n",
    "        # 10% of the time, keep original\n",
    "        if rng.random() < 0.5:\n",
    "          masked_token = tokens[index]\n",
    "        # 10% of the time, replace with random word\n",
    "        else:\n",
    "          masked_token = vocab_words[rng.randint(0, len(vocab_words) - 1)]\n",
    "\n",
    "      output_tokens[index] = masked_token\n",
    "\n",
    "      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))\n",
    "  assert len(masked_lms) <= num_to_predict\n",
    "  masked_lms = sorted(masked_lms, key=lambda x: x.index)\n",
    "\n",
    "  masked_lm_positions = []\n",
    "  masked_lm_labels = []\n",
    "  for p in masked_lms:\n",
    "    masked_lm_positions.append(p.index)\n",
    "    masked_lm_labels.append(p.label)\n",
    "\n",
    "  return (output_tokens, masked_lm_positions, masked_lm_labels)\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng):\n",
    "  \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "  while True:\n",
    "    total_length = len(tokens_a) + len(tokens_b)\n",
    "    if total_length <= max_num_tokens:\n",
    "      break\n",
    "\n",
    "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "    assert len(trunc_tokens) >= 1\n",
    "\n",
    "    # We want to sometimes truncate from the front and sometimes from the\n",
    "    # back to add more randomness and avoid biases.\n",
    "    if rng.random() < 0.5:\n",
    "      del trunc_tokens[0]\n",
    "    else:\n",
    "      trunc_tokens.pop()\n",
    "\n",
    "class TrainingInstance(object):\n",
    "  \"\"\"A single training instance (sentence pair).\"\"\"\n",
    "\n",
    "  def __init__(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,\n",
    "               is_random_next):\n",
    "    self.tokens = tokens\n",
    "    self.segment_ids = segment_ids\n",
    "    self.is_random_next = is_random_next\n",
    "    self.masked_lm_positions = masked_lm_positions\n",
    "    self.masked_lm_labels = masked_lm_labels\n",
    "\n",
    "  def __str__(self):\n",
    "    s = \"\"\n",
    "    s += \"tokens: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.tokens]))\n",
    "    s += \"segment_ids: %s\\n\" % (\" \".join([str(x) for x in self.segment_ids]))\n",
    "    s += \"is_random_next: %s\\n\" % self.is_random_next\n",
    "    s += \"masked_lm_positions: %s\\n\" % (\" \".join(\n",
    "        [str(x) for x in self.masked_lm_positions]))\n",
    "    s += \"masked_lm_labels: %s\\n\" % (\" \".join(\n",
    "        [tokenization.printable_text(x) for x in self.masked_lm_labels]))\n",
    "    s += \"\\n\"\n",
    "    return s\n",
    "\n",
    "  def __repr__(self):\n",
    "    return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = random.Random(RANDOM_SEED)\n",
    "instances = create_training_instances(\n",
    "    input_files, tokenizer, MAX_SEQ_LENGTH, DUPE_FACTOR,\n",
    "    SHROT_SEQ_PROB, MASKED_LM_PROB, MAX_PREDICTIONS,\n",
    "    rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_instance_to_example_files(instances, tokenizer, max_seq_length,\n",
    "                                    max_predictions_per_seq, output_files):\n",
    "  \"\"\"Create TF example files from `TrainingInstance`s.\"\"\"\n",
    "  writers = []\n",
    "  for output_file in output_files:\n",
    "    writers.append(tf.io.TFRecordWriter(output_file))\n",
    "\n",
    "  writer_index = 0\n",
    "\n",
    "  total_written = 0\n",
    "  for (inst_index, instance) in enumerate(instances):\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    segment_ids = list(instance.segment_ids)\n",
    "    assert len(input_ids) <= max_seq_length\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "      input_ids.append(0)\n",
    "      input_mask.append(0)\n",
    "      segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    masked_lm_positions = list(instance.masked_lm_positions)\n",
    "    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)\n",
    "    masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "    while len(masked_lm_positions) < max_predictions_per_seq:\n",
    "      masked_lm_positions.append(0)\n",
    "      masked_lm_ids.append(0)\n",
    "      masked_lm_weights.append(0.0)\n",
    "\n",
    "    next_sentence_label = 1 if instance.is_random_next else 0\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(segment_ids)\n",
    "    features[\"masked_lm_positions\"] = create_int_feature(masked_lm_positions)\n",
    "    features[\"masked_lm_ids\"] = create_int_feature(masked_lm_ids)\n",
    "    features[\"masked_lm_weights\"] = create_float_feature(masked_lm_weights)\n",
    "    features[\"next_sentence_labels\"] = create_int_feature([next_sentence_label])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "\n",
    "    writers[writer_index].write(tf_example.SerializeToString())\n",
    "    writer_index = (writer_index + 1) % len(writers)\n",
    "\n",
    "    total_written += 1\n",
    "\n",
    "    if inst_index < 20:\n",
    "      logging.info(\"*** Example ***\")\n",
    "      logging.info(\"tokens: %s\" % \" \".join(\n",
    "          [tokenization.printable_text(x) for x in instance.tokens]))\n",
    "\n",
    "      for feature_name in features.keys():\n",
    "        feature = features[feature_name]\n",
    "        values = []\n",
    "        if feature.int64_list.value:\n",
    "          values = feature.int64_list.value\n",
    "        elif feature.float_list.value:\n",
    "          values = feature.float_list.value\n",
    "        logging.info(\n",
    "            \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))\n",
    "\n",
    "  for writer in writers:\n",
    "    writer.close()\n",
    "\n",
    "  logging.info(\"Wrote %d total instances\", total_written)\n",
    "\n",
    "def create_int_feature(values):\n",
    "  feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "  return feature\n",
    "\n",
    "\n",
    "def create_float_feature(values):\n",
    "  feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))\n",
    "  return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:*** Writing to output files ***\n",
      "INFO:root:  pretraining_data/train_000.tfrecord\n",
      "INFO:root:  pretraining_data/train_001.tfrecord\n",
      "INFO:root:  pretraining_data/train_002.tfrecord\n",
      "INFO:root:  pretraining_data/train_003.tfrecord\n",
      "INFO:root:  pretraining_data/train_004.tfrecord\n",
      "INFO:root:  pretraining_data/train_005.tfrecord\n",
      "INFO:root:  pretraining_data/train_006.tfrecord\n",
      "INFO:root:  pretraining_data/train_007.tfrecord\n",
      "INFO:root:  pretraining_data/train_008.tfrecord\n",
      "INFO:root:  pretraining_data/train_009.tfrecord\n",
      "INFO:root:  pretraining_data/train_010.tfrecord\n",
      "INFO:root:  pretraining_data/train_011.tfrecord\n",
      "INFO:root:  pretraining_data/train_012.tfrecord\n",
      "INFO:root:  pretraining_data/train_013.tfrecord\n",
      "INFO:root:  pretraining_data/train_014.tfrecord\n",
      "INFO:root:  pretraining_data/train_015.tfrecord\n",
      "INFO:root:  pretraining_data/train_016.tfrecord\n",
      "INFO:root:  pretraining_data/train_017.tfrecord\n",
      "INFO:root:  pretraining_data/train_018.tfrecord\n",
      "INFO:root:  pretraining_data/train_019.tfrecord\n",
      "INFO:root:  pretraining_data/train_020.tfrecord\n",
      "INFO:root:  pretraining_data/train_021.tfrecord\n",
      "INFO:root:  pretraining_data/train_022.tfrecord\n",
      "INFO:root:  pretraining_data/train_023.tfrecord\n",
      "INFO:root:  pretraining_data/train_024.tfrecord\n",
      "INFO:root:  pretraining_data/train_025.tfrecord\n",
      "INFO:root:  pretraining_data/train_026.tfrecord\n",
      "INFO:root:  pretraining_data/train_027.tfrecord\n",
      "INFO:root:  pretraining_data/train_028.tfrecord\n",
      "INFO:root:  pretraining_data/train_029.tfrecord\n",
      "INFO:root:  pretraining_data/train_030.tfrecord\n",
      "INFO:root:  pretraining_data/train_031.tfrecord\n",
      "INFO:root:  pretraining_data/train_032.tfrecord\n",
      "INFO:root:  pretraining_data/train_033.tfrecord\n",
      "INFO:root:  pretraining_data/train_034.tfrecord\n",
      "INFO:root:  pretraining_data/train_035.tfrecord\n",
      "INFO:root:  pretraining_data/train_036.tfrecord\n",
      "INFO:root:  pretraining_data/train_037.tfrecord\n",
      "INFO:root:  pretraining_data/train_038.tfrecord\n",
      "INFO:root:  pretraining_data/train_039.tfrecord\n",
      "INFO:root:  pretraining_data/train_040.tfrecord\n",
      "INFO:root:  pretraining_data/train_041.tfrecord\n",
      "INFO:root:  pretraining_data/train_042.tfrecord\n",
      "INFO:root:  pretraining_data/train_043.tfrecord\n",
      "INFO:root:  pretraining_data/train_044.tfrecord\n",
      "INFO:root:  pretraining_data/train_045.tfrecord\n",
      "INFO:root:  pretraining_data/train_046.tfrecord\n",
      "INFO:root:  pretraining_data/train_047.tfrecord\n",
      "INFO:root:  pretraining_data/validate_000.tfrecord\n",
      "INFO:root:  pretraining_data/validate_001.tfrecord\n",
      "INFO:root:  pretraining_data/validate_002.tfrecord\n",
      "INFO:root:  pretraining_data/validate_003.tfrecord\n",
      "INFO:root:  pretraining_data/validate_004.tfrecord\n",
      "INFO:root:  pretraining_data/validate_005.tfrecord\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] no they re [MASK] know i mean business cause business [MASK] what i [MASK] [MASK] you insane you re [MASK] going over there you re gonna have a heart attack i don t care how [MASK] you didn t [MASK] this upset when they drove [MASK] car through the house why are you sid ##ing [MASK] them i [MASK] not sid ##ing with them [MASK] m worried about you don t worry about me sister girl cause my eyes are open for the first time you re not [SEP] the hell [MASK] m not they have to know i ll go over there [MASK] no [MASK] let me go okay i ll [MASK] them i ll handle them [MASK] got the wall up didn t [MASK] [SEP]\n",
      "INFO:root:input_ids: 2 29 52 31 4 38 6 133 381 376 381 4 17 6 4 4 5 1707 5 31 4 100 126 51 5 31 90 33 10 367 913 6 32 13 266 67 4 5 118 13 4 23 1300 93 52 2136 4 211 276 7 257 81 37 5 2358 35 4 104 6 4 36 2358 35 43 104 4 24 908 60 5 32 13 364 60 18 494 180 376 28 453 37 316 26 7 172 88 5 31 36 3 7 242 4 24 36 52 33 9 38 6 50 55 126 51 4 29 4 78 18 55 95 6 50 4 104 6 50 965 104 4 69 7 933 56 118 13 4 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 4 11 14 15 20 36 40 46 56 59 65 86 91 92 104 106 113 119 126 0\n",
      "INFO:root:masked_lm_ids: 90 21 133 37 36 63 48 7 43 24 6 5 242 6 29 855 965 6 6 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] ll hire both of you and i ll make it worth your [MASK] not unless you [MASK] gonna turn into a blonde a big screen tv and a roast beef sandwich i ll pay you [MASK] piece ##s each no thanks excuse shetland are you brain dead that s my ticket out of here i m not a mercenar [MASK] [MASK] want to start your own little army that gold buy ##s you [MASK] men you gonna let jubil in on where [MASK] re goin fifty sixty [MASK] northeast [MASK] here just off the turn ##pike hey what [MASK] of crap are you tryin to pull you ll be well [MASK] [SEP] the turn [MASK] this joker s tryin to take us to new york [MASK] [SEP]\n",
      "INFO:root:input_ids: 2 50 1455 404 16 5 15 6 50 119 11 773 27 4 36 866 5 4 90 345 177 10 3767 10 188 2220 722 15 10 4191 3849 2942 6 50 405 5 4 607 12 434 29 221 324 25911 37 5 992 249 14 8 28 1076 59 16 47 6 24 36 10 11655 4 4 73 9 288 27 248 123 1183 14 789 465 12 5 4 341 5 90 78 22130 19 25 96 4 31 963 2126 5396 4 9600 4 47 46 130 7 345 7795 105 17 4 16 1221 37 5 2966 9 571 5 50 34 74 4 3 7 345 4 23 9780 8 2966 9 98 94 9 183 972 4 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 4 13 17 21 36 43 55 60 61 74 79 83 88 90 99 111 115 123 126 0\n",
      "INFO:root:masked_lm_ids: 16 353 31 10 371 94 6 54 5 371 94 20 1247 16 239 1171 7795 9 631 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] we ##ugh he ##s [MASK] we go you cannot pass [MASK] do to me i have [MASK] do to don t hose me n do i [MASK] to hos ##maybe later [MASK] you were pretty your question chin ##ese last night poacher ##s we ##ugh he ##s dude we go you cannot pass wha ##we re dead i have to do to don t hose me n do i have to ##not so fast t [MASK] re you [MASK] [MASK] your question chin divorc last night poacher ##s [MASK] ##ugh he ##s dude we go you cannot pass wha ##we re romania i have [MASK] ##the full [MASK] ##hose me n captivate i hav [MASK] ##e african ostrich e [SEP] dad [SEP]\n",
      "INFO:root:input_ids: 2 20 13193 22 12 4 20 55 5 697 775 4 30 9 18 6 33 4 30 9 32 13 3756 18 626 30 6 4 9 14462 14244 402 4 5 109 337 27 490 3213 1180 175 160 9080 12 20 13193 22 12 899 20 55 5 697 775 3426 3104 31 249 6 33 9 30 9 32 13 3756 18 626 30 6 33 9 4005 44 687 13 4 31 5 4 4 27 490 3213 9719 175 160 9080 12 4 13193 22 12 899 20 55 5 697 775 3426 3104 31 6877 6 33 4 1643 516 4 10864 18 626 25504 6 8377 4 115 4946 9517 422 3 262 3 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:masked_lm_positions: 5 11 17 27 32 76 79 80 84 89 91 96 102 105 108 111 112 115 0 0\n",
      "INFO:root:masked_lm_ids: 51 17 9 33 31 231 109 337 1180 20 22 5 249 13 1894 626 30 24386 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] now we have a quo ##rum what happened there are factor pta of which [MASK] was not aware when [MASK] had you set them up wait you set them [MASK] ver [MASK] ##a wanted to go on [MASK] date [MASK] summer s dad [SEP] i don t know 2 000 unless of course you d prefer the fellatio ##n [MASK] ##fish sick twist ##ed low life [MASK] sucking pig at least [MASK] ll have someone to talk to what [MASK] s him the [MASK] one let s go i ll wait here but he ll see me you don t know what he s [MASK] chicken lndistinct god this is fun nobody touche ##s these [MASK] me bar towel hello captain tyler metal mammoth mini ##ng [SEP]\n",
      "INFO:root:input_ids: 2 68 20 33 10 13970 18516 17 275 51 37 2874 17676 16 256 4 39 36 1955 93 4 108 5 517 104 56 165 5 517 104 4 8003 4 164 259 9 55 25 4 717 4 1216 8 262 3 6 32 13 38 350 581 866 16 283 5 84 2149 7 25673 246 4 6731 667 2641 49 1175 156 4 4912 1471 64 536 4 50 33 255 9 169 9 17 4 8 62 7 4 65 78 8 55 6 50 165 47 45 22 50 77 18 5 32 13 38 17 22 8 4 1053 22189 144 23 21 487 458 16380 12 153 4 18 822 3107 235 568 2814 2779 24608 6520 1659 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 12 15 20 30 32 38 39 40 45 60 67 72 80 84 105 107 116 119 122 0\n",
      "INFO:root:masked_lm_ids: 12 6 6 56 15426 10 717 43 6 791 4583 6 14 2421 53 196 45 3107 2814 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] i m gonna i m gonna [MASK] m gonna go does this slide answer that [MASK] s my phone thank ##heeit who was it [MASK] hung up expecting a call explain how that s your [MASK] get her bag plane s waiting and i m not [SEP] small ##s [MASK] ##millan glen loch ##y laga see i can sometimes not pronounce them even [MASK] [MASK] [MASK] sober 15 years ago when i was here you had two choice ##s you had vat [UNK] or [MASK] had hai [MASK] [MASK] the gan ##tr ##ies hey i was expecting a phone call no luck in trac ##ing [MASK] then not [MASK] what s [MASK] d ##s doing [MASK] this place there s surrounding drugs in [MASK] than the [SEP]\n",
      "INFO:root:input_ids: 2 6 24 90 6 24 90 4 24 90 55 213 23 3404 492 14 4 8 28 395 150 12025 79 39 11 4 2475 56 2125 10 154 720 67 14 8 27 4 48 70 820 970 8 525 15 6 24 36 3 635 12 4 31265 8593 24301 54 20988 77 6 40 646 36 6264 104 152 4 4 4 6842 808 220 384 93 6 39 47 5 108 139 806 12 5 108 22520 1 103 4 108 13046 4 4 7 16966 15494 315 105 6 39 2125 10 395 154 29 662 19 19483 35 4 107 36 4 17 8 4 84 12 147 4 23 218 51 8 7033 1152 19 4 191 7 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 7 16 21 25 36 50 64 65 66 85 88 89 105 106 109 112 116 121 124 0\n",
      "INFO:root:masked_lm_ids: 6 14 5 52 381 1711 93 6 24 5 14320 68 35 14319 365 7 60 128 47 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] up to savior down in as little [MASK] [MASK] minute now the doctor he [MASK] be expensive i should know i [MASK] been under him several times soft ##ly very warm hands oh you re throwing it all won ##ki ##ly you do it then i can [MASK] bat and ##bowl oh [MASK] want a brother [MASK] bleat ##ing spirit ##ed conversation [MASK] shouting conversation [MASK] audible so nobody s ill no good makes [SEP] i get a lot [MASK] [MASK] blurt it all out then what s the trouble it [MASK] [MASK] hard to just blurt it out because of what mother said what did mother [MASK] that i wasn t [MASK] go [MASK] everyone we were poor but you re not everyone are you [SEP]\n",
      "INFO:root:input_ids: 2 56 9 7851 116 19 99 123 4 4 241 68 7 450 22 4 34 1937 6 134 38 6 4 111 400 62 1962 549 1166 124 137 1035 528 66 5 31 2789 11 42 173 7189 124 5 30 11 107 6 40 4 3040 15 30711 66 4 73 10 301 4 19625 35 1072 49 1819 4 1446 1819 4 8520 44 458 8 2191 29 75 506 3 6 48 10 244 4 4 12743 11 42 59 107 17 8 7 580 11 4 4 285 9 46 12743 11 59 143 16 17 271 140 17 80 271 4 14 6 290 13 4 55 4 398 20 109 724 45 5 31 36 398 37 5 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 3 8 9 15 22 48 53 54 57 63 66 80 81 92 93 108 113 115 124 0\n",
      "INFO:root:masked_lm_ids: 1004 99 10 50 72 13 6 73 5967 15 36 16 5161 8 922 106 9 599 398 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] what do [MASK] [MASK] is more excit ##ing having sex or stealin ##g [MASK] having sex or boost [MASK] cars um ooh [SEP] you ll get warm [MASK] s good i m warm now [SEP]\n",
      "INFO:root:input_ids: 2 17 30 4 4 21 128 2288 35 446 666 103 27635 1539 4 446 666 103 2863 4 1306 403 677 3 5 50 48 1035 4 8 75 6 24 1035 68 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 3 4 14 19 28 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 5 76 1306 35 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] who s eleanor it [MASK] a damn [MASK] [SEP] t you ever talk [MASK] my wife excuse [MASK] do [MASK] know teetotal otto is he s in the back in the paint shop [MASK] [MASK] you it s about time you came to see me h hey i missed you it s good to see you you look great it s good to see you you too ah you remember my ##juni ##e hey jun ##ie how are [MASK] we re doing good thankyou [MASK] [MASK] [MASK] look happy well [MASK] am happy i really am [MASK] what happened here what do you mean well it [MASK] nice but the chop [MASK] the strippe ##d cars my education what happened hey old age happened that s [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:input_ids: 2 79 8 3165 11 4 10 344 4 3 13 5 197 169 4 28 308 324 4 30 4 38 27273 2824 21 22 8 19 7 86 19 7 2135 1128 4 4 5 11 8 60 88 5 270 9 77 18 870 105 6 1057 5 11 8 75 9 77 5 5 91 163 11 8 75 9 77 5 5 120 437 5 219 28 31189 115 105 4223 247 67 37 4 20 31 147 75 2888 4 4 4 91 323 74 4 135 323 6 112 135 4 17 275 47 17 30 5 133 74 11 4 209 45 7 2770 4 7 7042 89 1306 28 3262 17 275 105 194 833 275 14 8 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 5 8 14 18 20 22 34 35 53 63 79 85 86 87 91 95 97 107 112 0\n",
      "INFO:root:masked_lm_ids: 8 211 60 18 5 96 21 14 75 9 5 75 75 5 6 112 44 8 1128 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] we [MASK] only following orders nothing more as vascula fellow soldier sure [MASK] you must understand [SEP] now you [MASK] no longer a soldier you are all war criminal [MASK] all [MASK] you lock them up nazi bastard ##s where s the guard reichs ##mars [MASK] herman ##n go ##ring at your service sir you are in [MASK] united states army stock ##ade at bad [MASK] ##rf [SEP]\n",
      "INFO:root:input_ids: 2 20 4 138 1869 1465 162 128 99 30089 2352 1190 148 4 5 184 240 3 68 5 4 29 879 10 1190 5 37 42 499 1581 4 42 4 5 1102 104 56 3781 638 12 96 8 7 1195 13082 15257 4 5460 246 55 3974 64 27 1042 179 5 37 19 4 1648 1406 1183 1506 6772 64 234 4 20029 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 2 9 13 20 30 32 46 58 65 66 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 109 10 124 31 12 16 16067 7 234 23750 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] i want to show you what i m really interested in [MASK] on [MASK] s go wow it looks like leftover got an a in [MASK] shop pam kevin laughing [MASK] it s always been kind of a hobby i whittle ##d that out of [MASK] wood huh it s beautiful yeah so what got you into [MASK] [MASK] ##ing [SEP] carpent [MASK] [SEP]\n",
      "INFO:root:input_ids: 2 6 73 9 264 5 17 6 24 112 1080 19 4 25 4 8 55 658 11 425 53 10067 69 97 10 19 4 1128 2159 1580 653 4 11 8 201 111 239 16 10 5158 6 15047 89 14 59 16 4 1100 280 11 8 423 71 44 17 69 5 177 4 4 35 3 19636 4 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 12 14 21 26 31 41 46 58 59 63 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 63 78 415 1100 71 15047 14970 158 7139 1310 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] don giovanni s ##lept with thousand ##s of women because he was afraid he wouldn [MASK] [MASK] loved by one [SEP] we re in for [MASK] night 12 [MASK] ##lem f rom [MASK] e at ball ##y s park place just a stone s throw down the board ##walk f 1850 where [MASK] new miss america was crown ##ed just a f ew [MASK] ago miss roche ##ster i ##tenb [MASK] was okay well uh roche ##ster s not a state but we [MASK] have somebody look that up [SEP]\n",
      "INFO:root:input_ids: 2 32 6928 8 1945 43 955 12 16 484 143 22 39 485 22 326 4 4 812 122 65 3 20 31 19 26 4 160 962 4 21515 532 8791 4 422 64 765 54 8 779 218 46 10 925 8 767 116 7 1225 12001 532 30069 96 4 183 330 1244 39 2404 49 46 10 532 5739 4 384 330 12370 7506 6 18924 4 39 95 74 158 12370 7506 8 36 10 733 45 20 4 33 415 91 14 56 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 8 16 17 26 28 29 30 33 51 53 64 70 71 84 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 16 13 34 7 962 814 12 70 8791 10 657 206 11 50 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] x it s an an ##agram rearrange ##d [MASK] [MASK] eric knox scra ##bble freak an old tunnel lead ##s down to the shore [MASK] sea approach is the best way to get [MASK] undetect ##ed we ll need a boat and a cover [MASK] incestuous t use the speedboat [SEP] isn t a chirop ##terate did [MASK] get the wrong guy it just hasn t changed form yet louis now [MASK] up wait a minute isn [MASK] this supposed to [MASK] within your juris ##di ##ction that [MASK] [MASK] regular human corpse in there i ve already checked the military personnel as well as the [MASK] employee ##s [MASK] what do you mean hey david they ve [MASK] [MASK] the public you want me to [SEP]\n",
      "INFO:root:input_ids: 2 1367 11 8 97 97 21129 18361 89 4 4 1252 3584 15784 24329 1185 97 194 2955 871 12 116 9 7 3749 4 826 1689 21 7 292 121 9 48 4 9153 49 20 50 127 10 988 15 10 894 4 24106 13 342 7 20876 3 226 13 10 23528 20083 80 4 48 7 236 195 11 46 1047 13 884 1172 365 2645 68 4 56 165 10 241 226 4 23 457 9 4 1335 27 20342 11006 18501 14 4 4 2250 644 2898 19 51 6 72 333 1567 7 2045 4606 99 74 99 7 4 3790 12 4 17 30 5 133 105 1012 52 72 4 4 7 1262 5 73 18 9 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 5 9 10 25 34 35 45 46 58 72 78 82 89 90 107 110 112 119 120 0\n",
      "INFO:root:masked_lm_ids: 97 11 8 10 19 9153 20 40 20 355 13 34 8 10 4499 1441 30 15114 89 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] or have such [MASK] mistress ##es gai ##us i think i understand my [MASK] people then perhaps caesar will be so good as to teach us out of his own extensive experience i call it love i am [MASK] [MASK] the people [MASK] my [MASK] i shall hold them [MASK] my bosom and [MASK] them tight [MASK] [SEP] hey hey hey [SEP]\n",
      "INFO:root:input_ids: 2 103 33 380 4 3484 216 5166 461 6 76 6 240 28 4 142 107 699 2232 82 34 44 75 99 9 1069 94 59 16 83 248 9201 1390 6 154 11 131 6 135 4 4 7 142 4 28 4 6 562 287 104 4 28 8801 15 4 104 1229 4 3 105 105 105 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 4 14 39 40 43 45 50 54 57 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 7489 248 187 227 37 632 9 3637 124 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] [MASK] to take a nice hot bath give myself [MASK] [MASK] you know how you do dr crane what happened to your [MASK] simon happened i ve caught his wretched [MASK] [MASK] m just going to go curl up in bed after all if [MASK] don t take care of myself who will make [MASK] some tea and honey won t you hey daph i thought you [MASK] [MASK] were watching the game down at [MASK] ##ty s nah the cable went out i [MASK] just watch it [SEP] he turk at the bar he met some [MASK] oh listen if you re going in the kitchen would you get [MASK] a beer he s got [MASK] real [MASK] for [MASK] friends and a great storyteller [SEP]\n",
      "INFO:root:input_ids: 2 4 9 98 10 209 565 2203 129 368 4 4 5 38 67 5 30 570 7058 17 275 9 27 4 2734 275 6 72 998 83 7501 4 4 24 46 100 9 55 8444 56 19 574 203 42 61 4 32 13 98 266 16 368 79 82 119 4 102 1101 15 482 173 13 5 105 5092 6 178 5 4 4 109 953 7 462 116 64 4 1349 8 2632 7 4059 291 59 6 4 46 351 11 3 22 3383 64 7 822 22 563 102 4 66 222 61 5 31 100 19 7 1524 110 5 48 4 10 1105 22 8 69 4 268 4 26 4 363 15 10 163 5963 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 1 10 11 20 23 31 32 45 55 68 69 76 85 91 98 111 117 119 121 0\n",
      "INFO:root:masked_lm_ids: 830 10 15030 275 3310 695 6 6 18 15 2734 8191 50 8 142 18 10 9111 527 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 0\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] what do ##ister do now pretend you [MASK] here after someone i don t get it [SEP] since that reindeer ran her down that [MASK] ##ful night [SEP]\n",
      "INFO:root:input_ids: 2 17 30 13912 30 68 1340 5 4 47 203 255 6 32 13 48 11 3 388 14 6851 1000 70 116 14 4 1537 160 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 3 8 21 25 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 6 270 1000 1723 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] you re not the boss of me now you re not the boss of me now you re not the boss of me now and you re not so big you re not [MASK] [MASK] of me now [MASK] re not the boss of me now [MASK] re not the [MASK] of me now and you re not so big [MASK] is unfair alarm [MASK] [MASK] [MASK] dewe ##y [MASK] up [SEP] yeah it sound ##s like they r e the [MASK] airline of the holy roman empir e [SEP]\n",
      "INFO:root:input_ids: 2 5 31 36 7 718 16 18 68 5 31 36 7 718 16 18 68 5 31 36 7 718 16 18 68 15 5 31 36 44 188 5 31 36 4 4 16 18 68 4 31 36 7 718 16 18 68 4 31 36 7 4 16 18 68 15 5 31 36 44 188 4 21 4769 1796 4 4 4 1817 54 4 56 3 71 11 387 12 53 52 736 422 7 4 4937 16 7 1009 2528 27976 422 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 5 34 35 39 46 47 51 59 61 65 66 67 70 82 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 718 7 718 5 68 5 718 44 156 1377 35 105 716 1801 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] claire [MASK] [MASK] interpret ##ive artist to a creative artist stay with [MASK] i need you we [MASK] to shoot in two days [MASK] it belongs to us [SEP] what does this mean [SEP]\n",
      "INFO:root:input_ids: 2 3377 4 4 8071 2295 1974 9 10 4171 1974 233 43 4 6 127 5 20 4 9 561 19 139 349 4 11 2656 9 94 3 17 213 23 133 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 2 3 13 18 24 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 99 97 18 288 107 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] interpret [MASK] as the wrong [MASK] it s it could be like a securit ##y alarm [MASK] it gets an incorrect response it defend ##s itself [MASK] what happens when it gets the right answer well i musso t [MASK] that phil but we gotta fii rise out luke what ifwe work [MASK] the other way [MASK] [MASK] gure out which tone ##s would be equivalent to the [MASK] chromosome ##s and then dub them into your recording ofthe signal [MASK] that possible yeah [MASK] i think so [MASK] good good what about [MASK] [MASK] gun will it accept the input so we can transmit a [MASK] ##d signal back to [SEP] they are sill ##y you are [MASK] broken [MASK] you are all alon ##e [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:input_ids: 2 8071 4 99 7 236 4 11 8 11 114 34 53 10 22258 54 1796 4 11 707 97 9809 3043 11 2559 12 1636 4 17 737 93 11 707 7 57 492 74 6 8171 13 4 14 1626 45 20 237 3887 1821 59 1752 17 5501 155 4 7 186 121 4 4 18244 59 256 2359 12 110 34 10681 9 7 4 13675 12 15 107 10380 104 177 27 4143 2032 1680 4 14 799 71 4 6 76 44 4 75 75 17 60 4 4 468 82 11 907 7 9358 44 20 40 7174 10 4 89 1680 86 9 3 52 37 18278 54 5 37 4 1143 4 5 37 42 17569 115 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 2 6 17 27 38 40 47 53 57 58 69 81 85 89 94 95 107 119 121 0\n",
      "INFO:root:masked_lm_ids: 49 492 93 44 32 38 5386 23 20 3887 1030 21 71 75 7 5394 1268 36 365 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] i want to have a [MASK] [SEP] [MASK] she wants to see her daughter [SEP]\n",
      "INFO:root:input_ids: 2 6 73 9 33 10 4 3 4 58 426 9 77 70 552 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_positions: 6 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_ids: 539 68 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:*** Example ***\n",
      "INFO:root:tokens: [CLS] go on go on i ll not have you fighting you know what happens when you fight [MASK] her stations sit down for males s sake want the money i [MASK] t fucked you i ll fight you for it you [MASK] me so that [MASK] the [MASK] of fight it ll be you want [MASK] stay down you want to [MASK] down get back down and fucking stay [MASK] i promise you [MASK] want to stay down dead ##ly kick [MASK] a fat fucker you know [MASK] cheek [MASK] bastard okay lads get him on his feet get back down or you will not be coming up next [SEP] he occupie not aware of the fact [MASK] [MASK] s just about to foul his [MASK] [SEP]\n",
      "INFO:root:input_ids: 2 55 25 55 25 6 50 36 33 5 1176 5 38 17 737 93 5 445 4 70 5602 389 116 26 14493 8 934 73 7 215 6 4 13 1144 5 6 50 445 5 26 11 5 4 18 44 14 4 7 4 16 445 11 50 34 5 73 4 233 116 5 73 9 4 116 48 86 116 15 200 233 4 6 584 5 4 73 9 233 116 249 124 1043 4 10 856 1907 5 38 4 3940 4 638 95 2767 48 62 25 83 740 48 86 116 103 5 82 36 34 251 56 273 3 22 6691 36 1955 16 7 612 4 4 8 46 60 9 4479 83 4 3\n",
      "INFO:root:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "INFO:root:masked_lm_positions: 18 20 24 31 42 46 48 56 62 70 74 82 88 90 107 112 118 119 126 0\n",
      "INFO:root:masked_lm_ids: 48 9 204 314 15 8 239 9 233 116 5 26 14 54 251 8 14 22 1132 0\n",
      "INFO:root:masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0\n",
      "INFO:root:next_sentence_labels: 1\n",
      "INFO:root:Wrote 2430308 total instances\n"
     ]
    }
   ],
   "source": [
    "TRAIN_FILE_NUMS = 48 # 6 * 8\n",
    "VAL_FILE_NUMS = 6\n",
    "output_files = [f\"{PRETRAINING_DIR}/train_{i:03d}.tfrecord\" for i in range(TRAIN_FILE_NUMS)]\n",
    "output_files += [f\"{PRETRAINING_DIR}/validate_{i:03d}.tfrecord\" for i in range(VAL_FILE_NUMS)]\n",
    "logging.info(\"*** Writing to output files ***\")\n",
    "for output_file in output_files:\n",
    "    logging.info(\"  %s\", output_file)\n",
    "write_instance_to_example_files(instances, tokenizer, MAX_SEQ_LENGTH,\n",
    "                                  MAX_PREDICTIONS, output_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import imp\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from model import models, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"base_model.json\") as fin:\n",
    "    options = json.load(fin)\n",
    "    \n",
    "# https://baekyeongmin.github.io/dev/tpu-recipe-2/\n",
    "\n",
    "def select_strategy(config) -> tf.distribute.Strategy:\n",
    "    \"\"\"\n",
    "    Configuration을 바탕으로 Strategy를 설정합니다.\n",
    "    :param config: Training / Inference Config\n",
    "    :returns: tf.distribute.Strategy\n",
    "    \"\"\"\n",
    "    if config[\"device\"] == \"GPU\":\n",
    "        devices: List[tf.config.PhysicalDevice] = tf.config.list_physical_devices(\"GPU\")\n",
    "        if len(devices) == 0:\n",
    "            raise RuntimeError(\"GPU를 찾지 못했습니다. 혹시 CUDA_VISIBLE_DEVICE를 제대로 설정하셨나요?\")\n",
    "        if len(devices) > 1:\n",
    "            strategy = tf.distribute.MirroredStrategy()\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(\"/gpu:0\")\n",
    "    elif config[\"device\"] == \"TPU\":\n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=os.environ[\"TPU_NAME\"])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.TPUStrategy(resolver)\n",
    "    else:\n",
    "        raise ValueError(f\"{config.device}는 지원되지 않는 기기입니다.\")\n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"device\": \"GPU\"\n",
    "}\n",
    "strategy = select_strategy(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.\\\\pretraining_data\\\\train_000.tfrecord', '.\\\\pretraining_data\\\\train_001.tfrecord', '.\\\\pretraining_data\\\\train_002.tfrecord', '.\\\\pretraining_data\\\\train_003.tfrecord', '.\\\\pretraining_data\\\\train_004.tfrecord', '.\\\\pretraining_data\\\\train_005.tfrecord', '.\\\\pretraining_data\\\\train_006.tfrecord', '.\\\\pretraining_data\\\\train_007.tfrecord', '.\\\\pretraining_data\\\\train_008.tfrecord', '.\\\\pretraining_data\\\\train_009.tfrecord', '.\\\\pretraining_data\\\\train_010.tfrecord', '.\\\\pretraining_data\\\\train_011.tfrecord', '.\\\\pretraining_data\\\\train_012.tfrecord', '.\\\\pretraining_data\\\\train_013.tfrecord', '.\\\\pretraining_data\\\\train_014.tfrecord', '.\\\\pretraining_data\\\\train_015.tfrecord', '.\\\\pretraining_data\\\\train_016.tfrecord', '.\\\\pretraining_data\\\\train_017.tfrecord', '.\\\\pretraining_data\\\\train_018.tfrecord', '.\\\\pretraining_data\\\\train_019.tfrecord', '.\\\\pretraining_data\\\\train_020.tfrecord', '.\\\\pretraining_data\\\\train_021.tfrecord', '.\\\\pretraining_data\\\\train_022.tfrecord', '.\\\\pretraining_data\\\\train_023.tfrecord', '.\\\\pretraining_data\\\\train_024.tfrecord', '.\\\\pretraining_data\\\\train_025.tfrecord', '.\\\\pretraining_data\\\\train_026.tfrecord', '.\\\\pretraining_data\\\\train_027.tfrecord', '.\\\\pretraining_data\\\\train_028.tfrecord', '.\\\\pretraining_data\\\\train_029.tfrecord', '.\\\\pretraining_data\\\\train_030.tfrecord', '.\\\\pretraining_data\\\\train_031.tfrecord', '.\\\\pretraining_data\\\\train_032.tfrecord', '.\\\\pretraining_data\\\\train_033.tfrecord', '.\\\\pretraining_data\\\\train_034.tfrecord', '.\\\\pretraining_data\\\\train_035.tfrecord', '.\\\\pretraining_data\\\\train_036.tfrecord', '.\\\\pretraining_data\\\\train_037.tfrecord', '.\\\\pretraining_data\\\\train_038.tfrecord', '.\\\\pretraining_data\\\\train_039.tfrecord', '.\\\\pretraining_data\\\\train_040.tfrecord', '.\\\\pretraining_data\\\\train_041.tfrecord', '.\\\\pretraining_data\\\\train_042.tfrecord', '.\\\\pretraining_data\\\\train_043.tfrecord', '.\\\\pretraining_data\\\\train_044.tfrecord', '.\\\\pretraining_data\\\\train_045.tfrecord', '.\\\\pretraining_data\\\\train_046.tfrecord', '.\\\\pretraining_data\\\\train_047.tfrecord']\n",
      "['.\\\\pretraining_data\\\\validate_000.tfrecord', '.\\\\pretraining_data\\\\validate_001.tfrecord', '.\\\\pretraining_data\\\\validate_002.tfrecord', '.\\\\pretraining_data\\\\validate_003.tfrecord', '.\\\\pretraining_data\\\\validate_004.tfrecord', '.\\\\pretraining_data\\\\validate_005.tfrecord']\n"
     ]
    }
   ],
   "source": [
    "data_pattern = f\"./{options['PRETRAINING_DIR']}/train*\"\n",
    "input_files = []\n",
    "input_files.extend(tf.io.gfile.glob(data_pattern))\n",
    "print(input_files)\n",
    "train_dataset = data.build_interleaved_tfrecord_dataset(input_files, 128, 20, 32, 4)\n",
    "train_dist_dataset  = strategy.experimental_distribute_dataset(train_dataset)\n",
    "\n",
    "data_pattern = f\"./{options['PRETRAINING_DIR']}/val*\"\n",
    "input_files = []\n",
    "input_files.extend(tf.io.gfile.glob(data_pattern))\n",
    "print(input_files)\n",
    "val_dataset = data.build_interleaved_tfrecord_dataset(input_files, 128, 20, 32, 4)\n",
    "val_dist_dataset  = strategy.experimental_distribute_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = models.BertPretrainModel(options, tf.keras.activations.gelu)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=1e-4, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}_{val_loss:0.3f}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\seokjong\\dev\\MyBert\\model\\models.py:106 call  *\n        loss_nsp = self.nsp(\n    C:\\Users\\seokjong\\dev\\MyBert\\model\\models.py:68 call  *\n        pooled_output = self.pooled(first_token_tensor)\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1006 __call__  **\n        with ops.name_scope_v2(name_scope):\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6650 __enter__\n        scope_name = scope.__enter__()\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\contextlib.py:113 __enter__\n        return next(self.gen)\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4241 name_scope\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\n\n    ValueError: 'bert_pretrain_model/nsp/pooling layer/' is not a valid scope name\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7ea5065309de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m model.fit(\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtrain_dist_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 725\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    726\u001b[0m             *args, **kwds))\n\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\seokjong\\dev\\MyBert\\model\\models.py:106 call  *\n        loss_nsp = self.nsp(\n    C:\\Users\\seokjong\\dev\\MyBert\\model\\models.py:68 call  *\n        pooled_output = self.pooled(first_token_tensor)\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1006 __call__  **\n        with ops.name_scope_v2(name_scope):\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6650 __enter__\n        scope_name = scope.__enter__()\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\contextlib.py:113 __enter__\n        return next(self.gen)\n    C:\\Users\\seokjong\\anaconda3\\envs\\tf_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:4241 name_scope\n        raise ValueError(\"'%s' is not a valid scope name\" % name)\n\n    ValueError: 'bert_pretrain_model/nsp/pooling layer/' is not a valid scope name\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_loss\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    train_dist_dataset, \n",
    "    epochs=1, \n",
    "    callbacks=callbacks, \n",
    "    steps_per_epoch=300, \n",
    "    validation_data=val_dist_dataset,\n",
    "    validation_steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_env1",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
