{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import sentencepiece as spm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as text\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# configure logging\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "# create formatter and add it to the handlers\n",
    "formatter = logging.Formatter('%(asctime)s :  %(message)s')\n",
    "sh = logging.StreamHandler()\n",
    "sh.setLevel(logging.INFO)\n",
    "sh.setFormatter(formatter)\n",
    "log.handlers = [sh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "Word Ids   : [101, 2023, 2003, 2107, 2019, 6429, 3185, 999, 102, 0, 0, 0]\n",
      "Input Mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "Type Ids   : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def set_tokenizer(tokenizer, max_len = 768):\n",
    "  def _set_tokenizer(sentence):\n",
    "    return tokenizer(\n",
    "        text = sentence,\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = max_len,           # Pad & truncate all sentences.\n",
    "        padding='max_length',\n",
    "        return_attention_mask = True   # Construct attn. masks.\n",
    "    )\n",
    "  return _set_tokenizer\n",
    "tokenizer = set_tokenizer(BertTokenizer.from_pretrained('bert-base-uncased'), 128)\n",
    "\n",
    "text_test = ['this is such an amazing movie!']\n",
    "hf_text_preprocessed = tokenizer(text_test)\n",
    "print(f'Keys       : {list(hf_text_preprocessed.keys())}')\n",
    "print(f'Word Ids   : {hf_text_preprocessed[\"input_ids\"][0][ :12]}')\n",
    "print(f'Input Mask : {hf_text_preprocessed[\"attention_mask\"][0][ :12]}')\n",
    "print(f'Type Ids   : {hf_text_preprocessed[\"token_type_ids\"][0][ :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLoss:\n",
    "    \"\"\"\n",
    "    Loss function suitable for BERT-like pretraining, that is, the task of pretraining a language model by combining\n",
    "    NSP + MLM. .. note:: Any label of -100 will be ignored (along with the corresponding logits) in the loss\n",
    "    computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_loss(self, labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        # make sure only labels that are not equal to -100\n",
    "        # are taken into account as loss\n",
    "        masked_lm_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), -100)\n",
    "        masked_lm_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[0], shape=(-1, shape_list(logits[0])[2])),\n",
    "            mask=masked_lm_active_loss,\n",
    "        )\n",
    "        masked_lm_labels = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"labels\"], shape=(-1,)), mask=masked_lm_active_loss\n",
    "        )\n",
    "        next_sentence_active_loss = tf.not_equal(tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), -100)\n",
    "        next_sentence_reduced_logits = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=logits[1], shape=(-1, 2)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        next_sentence_label = tf.boolean_mask(\n",
    "            tensor=tf.reshape(tensor=labels[\"next_sentence_label\"], shape=(-1,)), mask=next_sentence_active_loss\n",
    "        )\n",
    "        masked_lm_loss = loss_fn(y_true=masked_lm_labels, y_pred=masked_lm_reduced_logits)\n",
    "        next_sentence_loss = loss_fn(y_true=next_sentence_label, y_pred=next_sentence_reduced_logits)\n",
    "        masked_lm_loss = tf.reshape(tensor=masked_lm_loss, shape=(-1, shape_list(next_sentence_loss)[0]))\n",
    "        masked_lm_loss = tf.reduce_mean(input_tensor=masked_lm_loss, axis=0)\n",
    "\n",
    "        return masked_lm_loss + next_sentence_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(Layer):\n",
    "    \n",
    "    def __init__(self, model_opt: dict, **kwargs):\n",
    "        self.vocab_size = model_opt[\"vocab_size\"]\n",
    "        self.type_vocab_size = model_opt[\"type_vocab_size\"]\n",
    "        self.hidden_size = model_opt[\"hidden_size\"]\n",
    "        self.max_position_embeddings = model_opt[\"max_position_embeddings\"]\n",
    "        self.initializer_range = model_opt[\"initializer_range\"]\n",
    "        self.embeddings_sum = tf.keras.layers.Add()\n",
    "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=model_opt[\"norm_eps\"], name=\"LayerNorm\")\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=model_opt[\"dropout_rate\"])\n",
    "        super(EmbeddingLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self):\n",
    "        with tf.name_scope(\"wordpiece\"):\n",
    "            self.weight = self.add_weight(\n",
    "                name=\"weight\",\n",
    "                shape=[self.vocab_size, self.hidden_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "        with tf.name_scope(\"segment\"):\n",
    "            self.token_type_embeddings = self.add_weight(\n",
    "                name=\"embeddings\",\n",
    "                shape=[self.type_vocab_size, self.hidden_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"position\"):\n",
    "            self.position_embeddings = self.add_weight(\n",
    "                name=\"embeddings\",\n",
    "                shape=[self.max_position_embeddings, self.hidden_size],\n",
    "                initializer=get_initializer(self.initializer_range),\n",
    "            )\n",
    "\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(\n",
    "        self,\n",
    "        input_ids: tf.Tensor = None,\n",
    "        position_ids: tf.Tensor = None,\n",
    "        token_type_ids: tf.Tensor = None,\n",
    "        inputs_embeds: tf.Tensor = None,\n",
    "        training: bool = False,\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Applies embedding based on inputs tensor.\n",
    "        Returns:\n",
    "            final_embeddings (:obj:`tf.Tensor`): output embedding tensor.\n",
    "        \"\"\"\n",
    "        assert not (input_ids is None and inputs_embeds is None)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            inputs_embeds = tf.gather(params=self.weight, indices=input_ids)\n",
    "\n",
    "        input_shape = shape_list(inputs_embeds)[:-1]\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = tf.fill(dims=input_shape, value=0)\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)\n",
    "\n",
    "        position_embeds = tf.gather(params=self.position_embeddings, indices=position_ids)\n",
    "        position_embeds = tf.tile(input=position_embeds, multiples=(input_shape[0], 1, 1))\n",
    "        token_type_embeds = tf.gather(params=self.token_type_embeddings, indices=token_type_ids)\n",
    "        final_embeddings = self.embeddings_sum(inputs=[inputs_embeds, position_embeds, token_type_embeds])\n",
    "        final_embeddings = self.LayerNorm(inputs=final_embeddings)\n",
    "        final_embeddings = self.dropout(inputs=final_embeddings, training=training)\n",
    "\n",
    "        return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(Layer):\n",
    "    \"\"\"\n",
    "    TransformerEncoderLayer : MHSA -> residual sum, normalize -> FF\n",
    "    https://github.com/huggingface/transformers/blob/21e86f99e6b91af2e4df3790ba6c781e85fa0eb5/src/transformers/models/bert/modeling_tf_bert.py#L339\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Multihead self attention layer\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(Layer):\n",
    "    \"\"\"\n",
    "    개별 attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, model_opt: dict, **kwargs):\n",
    "        if model_opt[\"hidden_size\"] % model_opt[\"num_attention_heads\"] != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({model_opt[\"hidden_size\"]}) is not a multiple of the number \"\n",
    "                f\"of attention heads ({model_opt[\"num_attention_heads\"})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = model_opt[\"num_attention_heads\"]\n",
    "        self.attention_head_size = int(model_opt[\"hidden_size\"] / model_opt[\"num_attention_heads\"])\n",
    "        self.all_head_size = model_opt[\"num_attention_heads\"] * model_opt[\"attention_head_size\"]\n",
    "        self.sqrt_att_head_size = math.sqrt(model_opt[\"attention_head_size\"])\n",
    "\n",
    "        self.query = tf.keras.layers.Dense(\n",
    "            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n",
    "        )\n",
    "        self.key = tf.keras.layers.Dense(\n",
    "            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n",
    "        )\n",
    "        self.value = tf.keras.layers.Dense(\n",
    "            units=self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n",
    "        )\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n",
    "        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]\n",
    "        tensor = tf.reshape(tensor=tensor, shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
    "\n",
    "        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]\n",
    "        return tf.transpose(tensor, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        hidden_states: tf.Tensor,\n",
    "        attention_mask: tf.Tensor,\n",
    "        head_mask: tf.Tensor,\n",
    "        output_attentions: bool,\n",
    "        training: bool = False,\n",
    "    ) -> Tuple[tf.Tensor]:\n",
    "        batch_size = shape_list(hidden_states)[0]\n",
    "        mixed_query_layer = self.query(inputs=hidden_states)\n",
    "        mixed_key_layer = self.key(inputs=hidden_states)\n",
    "        mixed_value_layer = self.value(inputs=hidden_states)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        # (batch size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n",
    "        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)\n",
    "        attention_scores = tf.divide(attention_scores, dk)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
    "            attention_scores = tf.add(attention_scores, attention_mask)\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = tf.nn.softmax(logits=attention_scores, axis=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(inputs=attention_probs, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = tf.multiply(attention_probs, head_mask)\n",
    "\n",
    "        attention_output = tf.matmul(attention_probs, value_layer)\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # (batch_size, seq_len_q, all_head_size)\n",
    "        attention_output = tf.reshape(tensor=attention_output, shape=(batch_size, -1, self.all_head_size))\n",
    "        outputs = (attention_output, attention_probs) if output_attentions else (attention_output,)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_opt = {\n",
    "    \"vocab_size\" : 32000,\n",
    "    \"type_vocab_size\" : 2,\n",
    "    \"hidden_size\" : 128,\n",
    "    \"max_position_embeddings\" : 128,\n",
    "    \"initializer_range\" : 0.02,\n",
    "    \"norm_eps\" : 0.001,\n",
    "    \"dropout_rate\" : 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = EmbeddingLayer(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_text_preprocessed[\"input_ids\"]\n",
    "hf_text_preprocessed[\"attention_mask\"]\n",
    "hf_text_preprocessed[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EmbeddingLayer' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c9cbb795a022>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0memb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'EmbeddingLayer' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainBertModel(Model):\n",
    "    def __init__(self, model_opt, **kwargs):\n",
    "        super(PretrainBertModel, self).__init__(name='Pretrain', **kwargs)\n",
    "        self.embedding_layer = EmbeddingLayer(model_opt)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PretrainBertModel(model_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_position_embeddings = tf.get_variable(\n",
    "  name=\"test\",\n",
    "  shape=[128, 128],\n",
    "  initializer=tf.keras.initializers.TruncatedNormal())\n",
    "\n",
    "# Since the position embedding table is a learned variable, we create it\n",
    "# using a (long) sequence length `max_position_embeddings`. The actual\n",
    "# sequence length might be shorter than this, for faster training of\n",
    "# tasks that do not have long sequences.\n",
    "#\n",
    "# So `full_position_embeddings` is effectively an embedding table\n",
    "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n",
    "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n",
    "# perform a slice.\n",
    "position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n",
    "                             [seq_length, -1])\n",
    "num_dims = len(output.shape.as_list())\n",
    "\n",
    "# Only the last two dimensions are relevant (`seq_length` and `width`), so\n",
    "# we broadcast among the first dimensions, which is typically just\n",
    "# the batch size.\n",
    "position_broadcast_shape = []\n",
    "for _ in range(num_dims - 2):\n",
    "position_broadcast_shape.append(1)\n",
    "position_broadcast_shape.extend([seq_length, width])\n",
    "position_embeddings = tf.reshape(position_embeddings,\n",
    "                               position_broadcast_shape)\n",
    "output += position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.TruncatedNormal()\n",
    "full_position_embeddings = tf.Variable(\n",
    "  initializer(shape = (128, 128)),\n",
    "  name=\"test\")\n",
    "full_position_embeddings.shape\n",
    "position_embeddings = tf.slice(full_position_embeddings, [0, 0], [128, -1])\n",
    "position_embeddings.shape\n",
    "num_dims = 3\n",
    "position_broadcast_shape = []\n",
    "for _ in range(num_dims - 2):\n",
    "    position_broadcast_shape.append(1)\n",
    "    position_broadcast_shape.extend([128, 128])\n",
    "    position_embeddings = tf.reshape(position_embeddings,\n",
    "                                   position_broadcast_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128, 128), dtype=float32, numpy=\n",
       "array([[[ 0.01385949,  0.05264647,  0.06343699, ...,  0.09757596,\n",
       "         -0.03972503,  0.05346877],\n",
       "        [-0.03932196,  0.01364289,  0.03254468, ..., -0.02289535,\n",
       "          0.02897022,  0.04119596],\n",
       "        [ 0.0434962 ,  0.07995131,  0.01936734, ..., -0.02690749,\n",
       "          0.09007739, -0.04668866],\n",
       "        ...,\n",
       "        [-0.01632217,  0.02246051, -0.0352938 , ...,  0.00064092,\n",
       "         -0.00771515, -0.03805603],\n",
       "        [-0.02785651, -0.03085334,  0.06839695, ...,  0.00056849,\n",
       "         -0.0328668 , -0.01278302],\n",
       "        [ 0.00249366,  0.00049937,  0.07355256, ..., -0.03969698,\n",
       "         -0.09182624,  0.06338918]]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingLayer(Layer):\n",
    "    max_position_embeddings  = 512\n",
    "    hidden_size              = 128\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def _construct(self, **kwargs):\n",
    "        super()._construct(**kwargs)\n",
    "        self.embedding_table = None\n",
    "\n",
    "    # noinspection PyAttributeOutsideInit\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: () of seq_len\n",
    "        if input_shape is not None:\n",
    "            assert input_shape.ndims == 0\n",
    "            self.input_spec = keras.layers.InputSpec(shape=input_shape, dtype='int32')\n",
    "        else:\n",
    "            self.input_spec = keras.layers.InputSpec(shape=(), dtype='int32')\n",
    "\n",
    "        self.embedding_table = self.add_weight(name=\"embeddings\",\n",
    "                                               dtype=K.floatx(),\n",
    "                                               shape=[self.params.max_position_embeddings, self.params.hidden_size],\n",
    "                                               initializer=self.create_initializer())\n",
    "        super(PositionEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    # noinspection PyUnusedLocal\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # just return the embedding after verifying\n",
    "        # that seq_len is less than max_position_embeddings\n",
    "        seq_len = inputs\n",
    "\n",
    "        assert_op = tf.compat.v2.debugging.assert_less_equal(seq_len, self.params.max_position_embeddings)\n",
    "\n",
    "        with tf.control_dependencies([assert_op]):\n",
    "            # slice to seq_len\n",
    "            full_position_embeddings = tf.slice(self.embedding_table,\n",
    "                                                [0, 0],\n",
    "                                                [seq_len, -1])\n",
    "        output = full_position_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PositionEmbeddingLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
